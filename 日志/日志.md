[TOC]

Hadoop，spake了解，并学会使用如何搭建框架，

scala语言

hadoop00:刚建成的虚拟机



1.1虚拟环境搭建

1） 克隆虚拟机

 2） 修改克隆虚拟机的静态 ip

 3） 修改主机名







1.配置IP地址，在虚拟机创建完成之后先配置IP地址

```
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
IPADDR=192.168.202.100
GATEWAY=192.168.202.2
DNS1=192.168.202.2
```



(1) 关闭防火墙 

```
service iptables stop #关闭防火墙
chkconfig iptables off #关闭开机启动
service iptables status #查看是否关闭
chkconfig --list iptables

```

(2) 创建一般用户 

```
useradd hxh
passwd hxh
```

(3) 在opt下添加software和module文件夹并修改权限

```
[root@hadoop00 桌面]# mkdir /opt/software /opt/module
[root@hadoop00 桌面]# chown hxh:hxh /opt/software /opt/module
[root@hadoop00 桌面]# ll /opt/
```

（4）把这个用户加到sudoerssu

```
vim /etc/sudoers   #进入配置文件添加一行内容

## Allow root to run any commands anywhere 
root    ALL=(ALL)       ALL
hxh     ALL=(ALL)       NOPASSWD:ALL

修改后切换刚才的用户看是否修改成功
[root@hadoop00 桌面]# su hxh
[hxh@hadoop00 桌面]$ sudo ls
使用exit退回原来的用户
```

（5）修改host主机名方便识别

```
hostname hadoop101  #修改主机名
hostname     #查看是否修改成功

#修改主机映射文件（hosts 文件） 
# vim /etc/hosts 

添加如下内容 
192.168.1.100 hadoop100 
192.168.1.101 hadoop101 
192.168.1.102 hadoop102 
192.168.1.103 hadoop103 
192.168.1.104 hadoop104 
192.168.1.105 hadoop105 
192.168.1.106 hadoop106 
192.168.1.107 hadoop107 
192.168.1.108 hadoop108 
```





### 遇到的问题

#### 1.克隆的虚拟机会把原机的mac地址也克隆过来导致配置文件和激活的网卡不一致从而无法设置静态ip

```
1.输入命令ifconfig 和在 vim /etc/sysconfig/network-scripts/ifcfg-eth0 配置文件中配置的网卡信息不一致
2.修改70-persistent-net.rules文件（该文件用于保存历史网卡配置信息的）
vim /etc/udev/rules.d/70-persistent-net.rules
```

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170224134520787?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQXBvbGxvbl9rcmo=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

3. 将复制的mac地址复制到配置文件中

   ```
   vim /etc/sysconfig/network-scripts/ifcfg-eth0
   ```

4. 重启服务，如果报错就重启虚拟机

```
service network restart #重启网络
reboot #重启虚拟机
ifconfig #查看IP信息
```

#### 2.启动集群时DataNode无法启动

可能是多次格式化导致

namenode与datanode之间的不一致（多次格式化，版本不一致）

1. 先关闭dfs：sbin/stop-dfs.sh

2. 删除掉Hadoop下的dfs下的data里面的东西

3. 重启集群：./sbin/start-dfs.sh

4. 查看进程：jps

创建视图

```mysql
1. 单表上创建视图
create view view_name as 选择语句
如：
create view view1_emp as select cmpno,ename,job,mgr,hiredate,deptno from emp;
2. 多表上创建视图
create view view2_emp as select e.cmpno,e.ename,e.job,d.deptno.d.dname,d.loc from emp e inner join dept d on e.deptno=d.deptno;
或者
create view view2_emp as select e.cmpno,e.ename,e.job,d.deptno,d.dname,d.loc from emp e,dept d where e.deptno=d.deptno;
3.在其他视图上创建视图
create view view3_emp as select cmpno,ename,job,deptno,dname from view2_emp where loc='New York';
```

**注意**

1. 创建视图时的条件语句不能包含子查询
2. 引用的表不能是临时表
3. 在选择语句中不能引用系统变量或用户自定义变量

#### 查看试图

```mysql
show tables    #查看当前数据库中的表和视图
show table status like 'emp'  #查看视图当前状态
desc view1_emp			#查看试图的设计信息
show create view view_name #查看视图的定义信息
seleselect * from information_schema.views where table_name='view1_emp'    #通过视图表查看视图
```

#### 修改视图

```mysql
1. 使用create or replace 修改视图
create or replace view view1_emp as select ename,job,mgr,hiredate,deptno from emp;
2.使用alter view语句修改视图
alter view view_name as 选择语句
```

#### 删除视图

```mysql
drop view view_name;
```

#### 更新视图

更新视图其实是对基本表的增删更新

```mysql
insert into view1_emp values(8000,'Tom','analyst',7566,'1982-10-12',20);
向视图中插入一条数据会发现基本表中的数据也发生了变化，使用update和delete也是如此
```

##### 更新视图的限制条件

1. 视图中包含多行函数，如sum(),min(),max()

2. 视图中包含distinct，group by,having,union,/union all

3. 试图中的select语句包含子查询

4. 视图引用的只是文字值（也叫常量视图），如

   ```
   create view vview1 as select pi() pi;
   ```

5. 视图是根据不可更新视图构建的

6. 创建视图时指定了algorithm=temptable（这会将视图保存在一个临时表中所以无法更新）

7. 视图是连接视图

   ```mysqsl
   (1)使用insert更新连接视图时必须保证视图的所有组件都是可更新的
   （2）使用update更新连接视图时至少要保证组成该视图的一个基础视图是可以更新的，而且只能更新这个视图
   （3）连接视图不允许删除操作
   ```

   hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input/ output 'dfs[a-z.]+'

## 20.10.15

1. Google的三篇论文
2. 把Hadoop完全分布式部署好

### 用户管理

#### 1权限表

1. mysql.user

   储存了用户信息和全局的权限信息，可分为

   用户字段：包括host,user

   权限字段:以_priv结尾的字段，如select_priv,super_priv

   安全字段：储存安全信息

   资源控制字段：用来控制用户使用的资源，如： 	max_questions是每小时允许查询的最大次数

2. mysql.db

   某个用户对相关数据的权限可分为用户字段和权限字段

   用户字段：host,user,db（数据库名）

   权限字段：_priv结尾，这些只是数据库级的，与user表相比减少了与服务器管理相关的权限

3. 其他权限表

   tables_priv:表级权限表

   columns_priv:列级权限表

#### 2用户管理

##### 用户登录与退出MySQL数据库

用户登录

```mysql
mysql -h hostname|hostip -p port -u username -p  dbname -e sql语句
-h:主机名或IP地址
-p:端口号
-u:用户名
-p:密码
-e:执行的sql语句
```

退出

```
exit
quit
\q
```

##### 创建普通用户

create user(可同时创建多个用户)

```mysql
create user 'username'@'hostname'[identified by [password]'auth_string']['username'@'hostname'[,dentified by [password]auth_string]]
```

其中：

hostname为主机名，本机登录设置为localhost，远程登录设置为%或任意一台主机IP

identified by：设置密码，即为auth_string

password：用来对密码进行加密

##### 删除普通用户

删除普通用户，需要有全局级别的create user权限或对mysql的delete权限

```
1. drop user 'username'@'hostname'
2. delete from mysql.user where user='username' and host='hostname';
```

##### 修改密码

1. 修改root用户的密码

   ```mysql
   1. mysqladmin
   mysqladmin -u usename [-h hostname] -p password 'newpassword'
   2. root用户登录后使用set修改密码
   set password='new_password';
   ```

2. 用root用户修改普通用户密码

   ```mysql
   1. set password for 'username'@'hostname'='new_password';
   2. alter user 'username'@'hostname' identified by 'new_password';
   3.使用grant语句
   4.update 更新user表中的字段
   ```

3. 普通用户修改自己的秘密

   ```
   set password=new_passwoord
   ```

##### 找回密码

```
1. 停止mysql服务
在dos命令窗口关闭mysql服务
net stop MYSQL80;
2.创建一个文本文件，内涵一条密码修改命令
alter user 'root'@'localhost' identified by '123';
3. 执行上述密码修改命令文件
mysql --init-file='文件路径' --console
4.重启服务
net start MYSQL80
```

#### 3权限管理

##### 查看权限

```
show grants for 'username'@'hostname';
```

##### 授予权限

```mysql
grant priv_type on dbname.tb_name to 'username'@'hostname' with grant option;
#with grant option指的是有授予其他用户的权限的权利
```

##### 收回权限

```mysql
revoke priv_type on dbname.tb_name to 'username'@'hostname';
revoke priv_type on *.* to 'username'@'hostname';#*.*指所有数据库的所有表
revoke all，grantoptionfrom 'test4'@'localhost';
#收回该用户所有权限包括授予权限
```



1.输入命令ifconfig 和在 vim /etc/sysconfig/network-scripts/ifcfg-eth0 配置文件中配置的网卡信息不一致
2.修改70-persistent-net.rules文件（该文件用于保存历史网卡配置信息的）
vim /etc/udev/rules.d/70-persistent-net.rules
#删掉旧的信息，把eth1改为ech0,并赋值其mac地址
3.将复制的mac地址复制到配置文件中，并修改其静态IP
 vim /etc/sysconfig/network-scripts/ifcfg-eth0
 4.重启服务，如果报错就重启虚拟机

### 存储过程

是数据库中保存一系列生sql命令的集合

#### 使用存储过程

##### 创建存储过程

```mysql
#对表emp的雇员姓名列进行模糊检索的存储过程
delimiter $
create procedure sp_search_emp(in p_name varchar(20))
begin
	if p_name is null or p_name='' then 
	select * from emp;
	else 
  select * from emp where ename like p_name;
	end if;
end
$
delimiter;
call sp_search_emp(null);

delimiter：用来改变mysql监视器中使用sql语句分隔符的命令

```

##### 执行存储过程

```mysql
call sp_search_emp(null);

参数为空字符串也不能省略
```

##### 创建存储过程的要点

```mysql
1. 定义输出参数out 
delimiter $
create procedure sp_search_emp2(in p_name varchar(20),out p_cnt int)
begin
	if p_name is null or p_name='' then 
	select * from emp;
	else 
  select * from emp where ename like p_name;
	end if;
	select found_rows() into p_cnt;
end
$
call sp_search_emp2('%S%',@num);
select @num;

其中select... into...用于将select语句中取得的结果设置到指定的变量

2. 定义inout参数
调用程序可以传递参数，存储过程可以修改inout参数并将新值传递回调用程序
create procedure sp_counter1(inout count int(4) ,in inc int(4))
begin
	
		set count=count+inc;
end
$
delimiter;
set @counter=1;
call sp_counter1(@counter,5);
select @counter;
这里设置了@counter的值为1，经过存储过程的修改反回了新值，不使用inout则无法修改

3.使用if语句
if 条件 then
end if ;

if 条件 then
else
end if；

if 条件 then
elseif 条件 then
else
end if；
4. 使用case命令实现多重条件分支
case 变量
when 值 then 语句;
when 值 then 语句;
else 语句;
end case;

5. 定义本地变量
declare 变量名 类型 [default 默认值]
在声明了变量之后可以使用set 语句为变量赋值
如;set total_count=10;
这些声明的变量是有作用域的，在begin...end中声明的变量，作用域在这个范围之内
以@符号开头的变量是会话变量

6.使用循环
while 条件 do
代码
end while;

repeat
代码
until 条件 end repeat;

标签名：loop
  条件 leave 标签名
  代码
end loop;

```

#### 删除存储过程

```
drop procedure 存储过程名
```

### 游标

游标是一个存储在MYSQL服务器上的数据库查询，是被某条语句检索出来的结果集，它可以在结果集中灵活的操作每一行的内容。**游标是面向集合与面向行的设计思想之间的一种桥梁**

####  游标的使用步骤

```mysql
1. 创建游标
DECLARE <游标名> CURSOR FOR select 语句;
如：DECLARE mycursor CURSOR FOR select * from shops_info;
2.打开游标
open <游标名>
3.使用游标
declare 变量1 数据类型(与列值的数据类型相同)
declare 变量2 数据类型(与列值的数据类型相同)
declare 变量3 数据类型(与列值的数据类型相同)
FETCH [ NEXT | PRIOR | FIRST | LAST ] FROM <游标名> [ INTO 变量名1,变量名2,变量名3[,…] ]

#NEXT：取下一行的数据，游标一开始默认的第一行之前，故要让游标指向第一行，就必须第一次就执行FETCH NEXT操作
#INTO：将一行中每个对应的列下的数据放到与列的数据类型相同的变量中。
4.关闭游标
close mycursor;
5.释放游标
deallocate mycursor;

实例：
drop procedure if exists cursor_test;
delimiter //
create procedure cursor_test()
begin
     -- 声明与列的类型相同的四个变量
     declare id varchar (20);
     declare pname varchar (20);
     declare pprice varchar (20);
     declare pdescription varchar (20);
 
-- 1、定义一个游标mycursor
     declare mycursor cursor for
   select * from shops_info;
-- 2、打开游标
     open mycursor;
-- 3、使用游标获取列的值
     fetch  next from mycursor into id,pname,pprice,pdescription;
-- 4、显示结果
     select id,pname,pprice,pdescription;
-- 5、关闭游标
     close mycursor;
end ;
//
delimiter ;
call cursor_test();

这里只得到第一行的记录没有使用循环
```

### 存储函数

存储函数与存储过程基本相同，是由SQL语句和过程式语句组成的代码片段

不同点

| **存储函数**                             | **存储过程**                 |
| ---------------------------------------- | ---------------------------- |
| **不能拥有输出参数**                     | **可以拥有输出参数**         |
| **可以直接调用存储函数，不需要call语句** | **需要call语句调用存储过程** |
| **必须包含一条return语句**               | **不允许包含return语句**     |

```mysql
示例：在数据库中mysql_test中创建一个存储函数，要求该函数能根据给定的客户id号返回客户的性别，如果数据库中没有给定的客户id号，则返回"没有该客户".
用例表：customer（cust_id,cust_sex,cust_name）;

->use mysql_test;
->delimiter $$
->create function fn_name(cid int)   //创建一个存储函数，题目要求我们输入id号判断性别，所以是int
->        returns char(2)      //返回性别，所以char类型
->        deterministic       //为了提高where子句的性能加的
->begin                   //接下来要写函数体了
->       declare sex char(2);    //声明局部变量用来装性别
->       select cust_sex into sex from customer   //把这个性别放进局部变量
->             where cust_id = cid;    //判断id相符
->       if sex is null then          //判断这个局部变量性别对应的属性
->              return(select '没有该客户');
->       else if sex = '女' then
->              return(select "女");
->           else return(select "男");
->           end if;   //用来结束else if的语句
->      end if;   //用来结束if的语句
->end $$   //用来结束存储函数
```

#### 删除存储函数

```mysql
->drop function if exists fn_name $$

//指定要删除的存储函数的名称
```

参考：https://www.cnblogs.com/cheneyboon/p/11432095.html

#### 时间同步

1. 系统定时任务crontab

```
(1)启动crond 服务
service crond restart
(2)crontab -e   编辑crontab定时任务
   crontab -l	查询crontab定时任务
   crontab -r   删除当前用户所有的crontab任务
（3）参数说明：
 * * * * *  
 分，时，天，月，星期
 如：每天22点45分执行命令 45 22 * * * 命令
 每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字
*/1 * * * * /bin/echo ”11” >> /root/bailongma.txt

```

### 数据库三大范式

#### 第一范式（1NF）

数据库表的每一列都是不可分割的基本数据项，也就是确保每一列的原子性，如，对于地址这一属性就可以分割成省，市，详细地址等

#### 第二范式（2NF）

在满足第一范式的基础上，满足表中的每个实体（各记录行）必须可以被唯一的区分，为实现区分需要设置一个区分列，用以储存每个实体的唯一标识（主键），且实体的属性要完全依赖于主键（不能部分依赖）主要针对联合主键而言



#### 第三范式

在满足第二范式的基础之上，满足任意一个非主键字段不间接依赖主键字段

如：学生的姓名年龄依赖于学号，而学分依赖于课程姓名，并不依赖于学号

### idea 无法创建Scala class 选项解决办法汇总

https://www.cnblogs.com/lingluo2017/p/8673243.html

### 触发器

实现了针对表的自动管理，在事件发生时自动执行

#### 创建触发器

```mysql
CREATE TRIGGER trigger_name trigger_time trigger_event ON tb_name FOR EACH ROW trigger_stmt
trigger_name：触发器的名称
tirgger_time：触发时机，为BEFORE或者AFTER
trigger_event：触发事件，为INSERT、DELETE或者UPDATE
tb_name：表示建立触发器的表名，就是在哪张表上建立触发器
trigger_stmt：触发器的程序体，可以是一条SQL语句或者是用BEGIN和END包含的多条语句
所以可以说MySQL创建以下六种触发器：
BEFORE INSERT,BEFORE DELETE,BEFORE UPDATE
AFTER INSERT,AFTER DELETE,AFTER UPDATE

例如：
 DELIMITER ||
 CREATE TRIGGER demo BEFORE DELETE
 ON users FOR EACH ROW
 BEGIN
 INSERT INTO logs VALUES(NOW());
 INSERT INTO logs VALUES(NOW());
 END
 ||
DELIMITER ;
```

**tips：**一般情况下，mysql默认是以 ; 作为结束执行语句，与触发器中需要的分行起冲突

　　  为解决此问题可用DELIMITER，如：DELIMITER ||，可以将结束符号变成||

　　  当触发器创建完成后，可以用DELIMITER ;来将结束符号变成;

```
NEW.columnname：新增行的某列数据
OLD.columnname：删除行的某列数据
```

下面一个具体实例：

在emp表修改数据的时候在logs表中插入日志

```mysql
1.创建日志表
CREATE TABLE `logs` (
  `Id` int(11) NOT NULL AUTO_INCREMENT,
  `log` varchar(255) DEFAULT NULL COMMENT '日志说明',
  PRIMARY KEY (`Id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='日志表';

2.创建触发器
DELIMITER $
CREATE TRIGGER user_log AFTER INSERT ON emp FOR EACH ROW
BEGIN
DECLARE s1 VARCHAR(40)character set utf8;
DECLARE s2 VARCHAR(20) character set utf8;#后面发现中文字符编码出现乱码，这里设置字符集
SET s2 = " is created";
SET s1 = CONCAT(NEW.ename,s2);     #函数CONCAT可以将字符串连接
INSERT INTO logs(log) values(s1);
END $
DELIMITER ;

3.插入数据出发触发器
insert into emp values(7893,'king','president',null,'1981-11-17',5000,null,10);
select *from logs;
```

#### 删除触发器

```mysql
drop trigger 触发器名

查看触发器
show triggers 

```

**tips：**所有触发器信息都存储在information_schema数据库下的triggers表中

　　  可以使用SELECT语句查询，如果触发器信息过多，最好通过TRIGGER_NAME字段指定查询

### 数据的备份与还原

#### 数据的备份

mysqldump基本语法：

　　*mysqldump -u username -p dbname table1 table2 ...-> BackupName.sql*

　　其中：

- dbname参数表示数据库的名称；
- table1和table2参数表示需要备份的表的名称，为空则整个数据库备份；
- BackupName.sql参数表设计备份文件的名称，文件名前面可以加上一个绝对路径。通常将数据库被分成一个后缀名为sql的文件；

　　使用root用户备份test数据库下的person表

```
mysqldump -u root -p test person > D:\backup.sql
```

**备份多个数据库**

```mysql
mysqldump -u username -p --databases dbname2 dbname2 > Backup.sql
```

**备份所有数据库**

　　mysqldump命令备份所有数据库的语法如下：

```mysql
mysqldump -u username -p -all-databases > BackupName.sql
```

#### 恢复数据库

**mysql -u root -p [dbname] < backup.sq**

　　示例：

```mysql
mysql -u root -p test1 < C:\backup.sql
```

或者在已经登陆MySQL的情况下

```mysql
use db_name;
source 备份的数据库文件路径;
```

## 常用快捷键

**Ctrl+Shift+J：将选中的行合并成一行**

**Ctrl+Shift+-：收缩所有代码**

**Ctrl+Shitft+向下箭头：将光标所在的代码块向下整体移动**

**Ctrl+Alt+I：自动缩进行**

**Ctrl+]：快速跳转至诸如{}围起来的代码块的结尾处**

**Ctrl+[：快速跳转至诸如{}围起来的代码块的开头处**

**Ctrl+向左箭头：将光标移至前一个单词**

# leetcode100

## 01 两数之和

解法一：暴力解法

```java
public int[] twoSum(int[] nums, int target) {
        for(int i =0;i<nums.length;i++){
            for(int j =i+1;j<nums.length;j++){
                if(nums[i]+nums[j]==target){
                    return new int[]{i,j};
                }
            }
        }
        return new int[]{-1};

    }
```

解法二：哈希表

```java
   public int[] twoSum(int[] nums, int target) {
        Map<Integer, Integer> hashtable = new HashMap<Integer, Integer>();
        for (int i = 0; i < nums.length; ++i) {
            if (hashtable.containsKey(target - nums[i])) {
                return new int[]{hashtable.get(target - nums[i]), i};
            }
            hashtable.put(nums[i], i);
        }
        return new int[0];
    }
```

- i++ 即后加加，原理是：先自增，然后返回自增之前的值
- ++i 即前加加，原理是：先自增，然后返回自增之后的值

[i++与++i详解](https://blog.csdn.net/android_cai_niao/article/details/106027313)

在**Java中i++语句是需要一个临时变量取存储返回自增前的值，而++i不需要**。这样就导致使用i++时系统需要先申请一段内存空间，然后将值赛如进去，最后不用了才去释放。

hashmap原理详解：https://blog.csdn.net/woshimaxiao1/article/details/83661464

## 02 有序数组两数之和

解法一：二分法

```java
class Solution {
    public int[] twoSum(int[] numbers, int target) {
        for (int i = 0; i < numbers.length; ++i) {
            int low = i + 1, high = numbers.length - 1;
            while (low <= high) {
                int mid = (high - low) / 2 + low;
                if (numbers[mid] == target - numbers[i]) {
                    return new int[]{i + 1, mid + 1};
                } else if (numbers[mid] > target - numbers[i]) {
                    high = mid - 1;
                } else {
                    low = mid + 1;
                }
            }
        }
        return new int[]{-1, -1};
    }
}

```

解法二：双指针

```java
class Solution {
    public int[] twoSum(int[] numbers, int target) {
        int low = 0, high = numbers.length - 1;
        while (low < high) {
            int sum = numbers[low] + numbers[high];
            if (sum == target) {
                return new int[]{low + 1, high + 1};
            } else if (sum < target) {
                ++low;
            } else {
                --high;
            }
        }
        return new int[]{-1, -1};
    }
}

```



# 大数据学习流程

参考学习路线;[https://github.com/heibaiying/BigData-Notes/blob/master/notes/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF.md](https://github.com/heibaiying/BigData-Notes/blob/master/notes/大数据学习路线.md)

## 一、大数据处理流程

[![img](https://camo.githubusercontent.com/42f5a32088d584b620ac42f4cae95ece19e09ced711123e5d23677c674b58b95/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732fe5a4a7e695b0e68daee5a484e79086e7ae80e58c96e6b581e7a88b2e706e67)](https://camo.githubusercontent.com/42f5a32088d584b620ac42f4cae95ece19e09ced711123e5d23677c674b58b95/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732fe5a4a7e695b0e68daee5a484e79086e7ae80e58c96e6b581e7a88b2e706e67)

### 1.1 数据收集

大数据处理的第一步是数据的收集。现在的中大型项目通常采用微服务架构进行分布式部署，所以数据的采集需要在多台服务器上进行，且采集过程不能影响正常业务的开展。基于这种需求，就衍生了多种日志收集工具，如 Flume 、Logstash、Kibana 等，它们都能通过简单的配置完成复杂的数据收集和数据聚合。

### 1.2 数据存储

收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。

分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。

### 1.3 数据分析

大数据处理最重要的环节就是数据分析，数据分析通常分为两种：批处理和流处理。

- **批处理**：对一段时间内海量的离线数据进行统一的处理，对应的处理框架有 Hadoop MapReduce、Spark、Flink 等；
- **流处理**：对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的处理框架有 Storm、Spark Streaming、Flink Streaming 等。

批处理和流处理各有其适用的场景，时间不敏感或者硬件资源有限，可以采用批处理；时间敏感和及时性要求高就可以采用流处理。随着服务器硬件的价格越来越低和大家对及时性的要求越来越高，流处理越来越普遍，如股票价格预测和电商运营数据分析等。

上面的框架都是需要通过编程来进行数据分析，那么如果你不是一个后台工程师，是不是就不能进行数据的分析了？当然不是，大数据是一个非常完善的生态圈，有需求就有解决方案。为了能够让熟悉 SQL 的人员也能够进行数据的分析，查询分析框架应运而生，常用的有 Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 等。这些框架都能够使用标准的 SQL 或者 类 SQL 语法灵活地进行数据的查询分析。这些 SQL 经过解析优化后转换为对应的作业程序来运行，如 Hive 本质上就是将 SQL 转换为 MapReduce 作业，Spark SQL 将 SQL 转换为一系列的 RDDs 和转换关系（transformations），Phoenix 将 SQL 查询转换为一个或多个 HBase Scan。

### 1.4 数据应用

数据分析完成后，接下来就是数据应用的范畴，这取决于你实际的业务需求。比如你可以将数据进行可视化展现，或者将数据用于优化你的推荐算法，这种运用现在很普遍，比如短视频个性化推荐、电商商品推荐、头条新闻推荐等。当然你也可以将数据用于训练你的机器学习模型，这些都属于其他领域的范畴，都有着对应的框架和技术栈进行处理，这里就不一一赘述。

#### 1. 框架分类

上面我们介绍了很多大数据框架，这里进行一下分类总结：

**日志收集框架**：Flume、Logstash、Filebeat

**分布式文件存储系统**：Hadoop HDFS

**数据库系统**：Mongodb、HBase

**分布式计算框架**：

- 批处理框架：Hadoop MapReduce
- 流处理框架：Storm
- 混合处理框架：Spark、Flink

**查询分析框架**：Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix

**集群资源管理器**：Hadoop YARN

**分布式协调服务**：Zookeeper

**数据迁移工具**：Sqoop

**任务调度框架**：Azkaban、Oozie,Cascading,Hamake

**集群部署和监控**：Ambari、Cloudera Manager

分布式是“并联”工作的，集群是“串联”工作的

并发：

同一个CPU执行多个任务，按细分的时间片交替执行

并行：

在多个CPU上同时处理多个任务

# spark

## 历史简介

hadoop1.0版本

![](C:\Users\yzy\Desktop\日志\image-20201021101621597.png)

文件系统核心模块(HDFS)：
**NameNode**：集群当中的主节点，主要用于管理集群当中的各种数据
**secondaryNameNode**：主要能用于hadoop当中元数据信息的辅助管理
**DataNode**：集群当中的从节点，主要用于存储集群当中的各种数据
数据计算核心模块(MapReduce)：
**JobTracker**：接收用户的计算请求任务，并分配任务给从节点
**TaskTracker**：负责执行主节点JobTracker分配的任务

2.0版本

![image-20201021102708372](C:\Users\yzy\Desktop\日志\image-20201021102708372.png)

文件系统核心模块(HDFS)：
**NameNode**：集群当中的主节点，主要用于管理集群当中的各种数据
**secondaryNameNode**：主要能用于hadoop当中元数据信息的辅助管理
**DataNode**：集群当中的从节点，主要用于存储集群当中的各种数据
资源调度（yarn）：
**ResourceManager**：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分

在rm中加了一层	AM(Application Master) ,通过am调用Driver

**NodeManager**：负责执行主节点ResourceManager分配的任务

提出yarn,降低其耦合度

其中包含Container,将资源和计算分开降低耦合性，Container里面放task

**spark**

![image-20201021105448735](C:\Users\yzy\Desktop\日志\image-20201021105448735.png)

2.2.1Master
Spark 特有资源调度系统的 Leader。掌管着整个集群的资源信息，类似于 Yarn 框架中的 ResourceManager，主要功能：

1. 监听 Worker，看 Worker 是否正常工作；

Master 对 Worker、Application 等的管理(接收 Worker 的注册并管理所有的Worker，**接收 Client 提交的 Application，调度等待的 Application 并向Worker 提交**)。
2.2.2Worker
Spark 特有资源调度系统的 Slave，有多个。每个 Slave 掌管着所在节点的资源信息，类似于 Yarn 框架中的 NodeManager，主要功能：

1. 通过 RegisterWorker 注册到 Master；
2. 定时发送心跳给 Master；
3. **根据 Master 发送的 Application 配置进程环境，并启动 ExecutorBackend**(执行 Task 所需的临时进程)

### hadoop生态各部分的作用

Hadoop 中各模块的作用： 

1、Hadoop HDFS为HBase提供了高可靠性的底层存储支持。

2、Hadoop MapReduce为HBase提供了高性能的计算能力。

3、Zookeeper为HBase提供了稳定服务和failover机制。

4、Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变得非常简单。

5、Sqoop则为HBase提供了方便的RDBMS（关系型数据库）数据导入功能，使得传统数据库数据向HBase中迁移变得非常方便。

spark的数据存储是根据分区数决定的，文件的字节数决定的

## Spark的工作流程

### 基本概念的理解

在开始之前需要先了解Spark中Application，Job，Stage等基本概念，官方给出的解释如下表：

| Term            | Meaning                                                     |
| :-------------- | :---------------------------------------------------------- |
| Application     | 用户编写的Spark应用程序,包括一个Driver和多个executors       |
| Application jar | 包含用户程序的Jar包                                         |
| Driver Program  | 运行main()函数并创建SparkContext**进程**                    |
| Cluster manager | 在集群上获取资源的外部服务，如standalone manager,yarn,Mesos |
| deploy mode     | 部署模式，区别在于driver process运行的位置                  |
| worker node     | 集群中可以运行程序代码的节点（机器）                        |
| Executor        | 运行在worker node上执行具体的计算任务，存储数据的**进程**   |
| Task            | 被分配到一个Executor上的计算单元                            |
| Job             | 由多个任务组成的并行计算阶段，因RDD的Action产生             |
| Stage           | 每个Job被分为小的计算任务组，每组称为一个stage              |
| DAGScheduler    | 根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler       |
| TaskScheduler   | 将TaskSet提交给worker运行，每个executor运行什么task在此分配 |

作者：由木人_番茄
链接：https://www.jianshu.com/p/3aa52ee3a802

https://www.cnblogs.com/ITtangtang/p/7967386.html

**job**

 一个spark的程序可以被划分为多个job，划分的依据是每遇到一个Action操作就生成一个新的job，

**Stage**

每个job又可以划分为一个或多个可以并行计算的Stage,划分的依据是是否进行了shuffle操作（即根据rdd之间的dependency关系），宽依赖时会产生shuffle操作，会划分为不同的stage

**Task**

***stage是由task组成的并行计算***(根据分区交由不同的executer计算)，一般一个rdd的分区对应一个task,task又分为ResultTask和ShuffleMapTask。经过shuffle后rdd的分区改变，task数也会改变。

**partition**

通常一个RDD被划分为一个或多个Partition，Partition是Spark进行数据处理的基本单位，**一般来说一个Partition对应一个Task，而一个Partition中通常包含数据集中的多条记录(Record)**。
 注意不同Partition中包含的记录数可能不同。Partition的数目可以在创建RDD时指定，也可以通过reparation和coalesce等算子重新进行划分。

**master和worker**

一个集群可能有多个master节点和多个worker节点，

master节点常驻master守护进程，负责管理worker节点，我们从master节点提交应用。

worker节点常驻worker守护进程，与master节点通信，并且管理executor进程。

一台机器可以同时作为master和worker节点

**driver和executor进程**

1.driver进程就是应用的main()函数并且构建sparkContext对象，当我们提交了应用之后，便会启动一个对应的driver进程，driver本身会根据我们设置的参数占有一定的资源（主要指cpu core和memory）

2.driver可以运行在master上，也可以运行worker上（根据部署模式的不同）。driver首先会向集群管理者（standalone、yarn，mesos）申请spark应用所需的资源，也就是executor，然后集群管理者会根据spark应用所设置的参数在各个worker上分配一定数量的executor，每个executor都占用一定数量的cpu和memory

3.在申请到应用所需的资源以后，driver就开始调度和执行我们编写的应用代码了。driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。

4.executor进程宿主在worker节点上，一个worker可以有多个executor。每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回给driver，每个executor执行的task都属于同一个应用

也就是说我们在提交代码到集群上并启动执行之后会执行以下操作：

启动driver ===> driver向集群管理者申请executer ===> Cluster manager会根据参数在各个worker上分配一定数量的executer ===>driver将代码拆分为stage,每个stage又由多个task组成 ====>将task分配给executer执行 ===> 执行完结果返回给driver

worker与executer是一对多

executer与task是一对一的关系

 **宽依赖**

具有宽依赖的 transformations 包括: sort, reduceByKey, groupByKey, join, 和调用rePartition函数的任何操作

### actions操作

获取元素
collect(), first(), take(*n*), takeSample(withReplacement, num, [seed]),  takeOrdered(n, [ordering])

计数元素
count(), countByKey()

迭代元素
reduce(*func*),  foreach(*func*)

保存元素
saveAsTextFile(*path*), saveAsSequenceFile(*path*), saveAsObjectFile(*path*)
链接：https://www.imooc.com/article/267796

```
观察函数的返回类型：如果返回的是  RDD 类型，那么这是 transformation; 如果返回的是其他数据类型，那么这是 action.
```

2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_37_2019-07-17 00:00:02_手机_-1_-1_null_null_null_null_3
 2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_48_2019-07-17 00:00:10_null_16_98_null_null_null_null_19
 2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_6_2019-07-17 00:00:17_null_19_85_null_null_null_null_7

### 算子的分类

1、Trasformtion算子
union、reduceByKey、groupBy、join、map、mapPartition
、cogroup、parallelize、textFile、leftoutJoin、flatMap、coalesce、Repartition

2、Action算子（action时触发job）
count、take、collect、foreach、foreachPartition、saveAsTextFile、ditinct、
first、reduce

3、shuffle算子（shuffer时产生stage）
reduceByKey、groupBy、join、coalesce、Repartition、leftoutJoin、cogroup

4、产生Stage划分的算子
reduceByKey、groupBy、join、coalesce、Repartition、leftoutJoin、cogroup

5、触发Job算子

count、take、collect、foreach、foreachPartition、saveAsTextFile、ditinct、
first、reduce

[各算子的介绍](https://blog.csdn.net/weixin_38750084/article/details/83145432#%EF%BC%8820%EF%BC%89join)

https://blog.csdn.net/do_what_you_can_do/article/details/53155555

```json
"x":116.321551,"y":40.008945
"x":116.323722,"y":40.008761
```

使用zippartions的话每个分区第一个会自动分隔，那如何设置分区呢？

根据自定义的时间切分？怎么切？

要解决的问题：

1. join的优化（完成）

2. 输出多边形，输出中心点

3. 累加器与预停留点有序无序问题（完成）

4. 预留参数

5. 读入整个数据集（完成）

   （1）用正则表达式筛选文件

   （2）每个文件存到一个数组里

   （3）自定义分区，根据用户和天数的不同
   
6. 怎样按分区输出呢？

不足之处在哪？



### 矢量数据的存储

1. 如何让矢量数据均匀的分布

2. 矢量数据的格式和存储方式，可以把矢量数据根据范围划分成网格，储存每个网格的图形数，点数，达到负载均衡，计算每个节点分布多少个格子。

   

## RDD编程

### RDD的创建

1. 从集合中创建（两种方式）

```scala
 val config = new SparkConf().setMaster("local[*]").setAppName("RDD01")
    val sc = new SparkContext(config)
   //rdd默认的分区是由内核数决定的
    val listRDD = sc.makeRDD(List(1, 2, 3, 4))
    val arrayRDD = sc.parallelize(Array(1, 2, 3, 4))
    listRDD.collect().foreach(println)
    
    arrayRDD.collect().foreach(println)
```

2. 从外部存储中创建

```scala
	//2.从外部存储中创建
    val value = sc.textFile("in",2)
    value.collect().foreach(println)
    value.saveAsTextFile("output")
```

3. 从其他 RDD 转换得到新的 RDD

就是通过 RDD 的各种转换算子来得到新的 RDD.

### RDD的转换

#### value类型

##### 2.3.1.1   map(func)

作用: 返回一个新的 RDD, 该 RDD 是由原 RDD 的每个元素经过函数转换后的值而组成. 就是对 RDD 中的数据做转换.

创建一个包含1-10的的 RDD，然后将每个元素*2形成新的 RDD

```scala
	val value = sc .makeRDD(1 to 10)
    val value1 = value.map(_* 2)
    value1.collect().foreach(println)
```

##### mapPartitions

遍历RDD中所有的分区

效率优于map，减少了发送到执行器执行的交互次数

可能会出现内存溢出（oom）

```scala
 	val value = sc .makeRDD(1 to 10)
    val value1 = value.mapPartitions(datas=>{
      datas.map(_*2)
    })
    value1.collect().foreach(println)
```

##### mapPartitionsWithIndex

遍历分区，且拿到分区号

```scala
 val value = sc .makeRDD(1 to 10,2)
    val value1 = value.mapPartitionsWithIndex((num,datas)=>{
      datas.map((_,"分区号"+num))
    })
    value1.collect().foreach(println)
  }
输出结果为：
(1,分区号0)
(2,分区号0)
(3,分区号0)
(4,分区号0)
(5,分区号0)
(6,分区号1)
(7,分区号1)
(8,分区号1)
(9,分区号1)
(10,分区号1)
```

##### flatMap(func)

将func应用到rdd的所有元素后，并对rdd进行扁平化处理

```scala
 val value = sc .makeRDD(Array(List(1,2),List(3,4)))
    val value1 = value.flatMap(datas=>datas)
    value1.collect().foreach(println)
```

##### glom()

将每个分区的元素合并成一个数组，如果想对各个分区中的数据进行操作，用它比较方便

```scala
//将各个分区的内容打印出来   
val value = sc .makeRDD(1 to 16 ,4 )
    val value1 = value.glom()
    value1.collect().foreach(array=>{println(array.mkString(","))})
```

##### groupBy

按照func的返回值对rdd进行分组

```scala
    //分组后形成了对偶元组，k表示分组的key，v表示分组的数据集合
    val value = sc .makeRDD(1 to 16 )
    val value1 = value.groupBy(i=>i%2)
    value1.collect().foreach(println)
```

##### 2.3.1.1   filter(func)

作用: 过滤. 返回一个新的 RDD 是由func的返回值为true的那些元素组成

```scala
	val value = sc .makeRDD(1 to 16 )
    val value1 = value.filter(i=>i%2==0)
    value1.collect().foreach(println)
```

##### sample

抽样sample(withReplacement, fraction, seed)

withReplacement:是否可放回

fraction：抽样比例

seed：种子数

```scala
	val value = sc .makeRDD(1 to 16 )
    val value1 = value.sample(false,0.5 ,1)
    value1.collect().foreach(println)
```

#####  coalesce(numPartitions，shuffle)

作用: 缩减分区数到指定的数量，用于大数据集过滤后，提高小数据集的执行效率。

```scala
	val value = sc .makeRDD( 1 to 16 ,4)
    val value1 = value.coalesce(3)
    value.saveAsTextFile("output")
```

##### repartition(numPartitions)

作用: 根据新的分区数, 重新 shuffle 所有的数据, 这个操作总会通过网络.

新的分区数相比以前可以多, 也可以少

repartition其实就是对coalesce进行了封装，将shuffle设置为了true

#####  sortBy(func,[ascending], [numTasks])

作用: 使用func先对数据进行处理，按照处理后结果排序，默认为正序。numTasks:分区数

```scala
    val value = sc .makeRDD( 1 to 16 )
    val value1 = value.sortBy((num=>num),false,2)
    value1.saveAsTextFile("output")
```

#### 双value类型

#### k-v类型



### spark中对于文件类型分区数，和存储的文件数的确定的确定

如 在sc.textFile中设置分区数，假设一个文件13个字节。

1. 若为设置的分区数为2则用13%2=6,6,1。所以会产生三个文件用于存储数据（但是并不是每个文件中都是有内容的）。**当字节总数除以分区数大于9时则最终产生的存储数据的文件数=分区数，不再考虑余数**（这里不太理解，不知道为啥）。

2. 每个文件中存入的内容要大于相应的字节数（阈值，第一个文件要大于6，所以至少为7）。
3. spark会按行读取，无论该行数据量为多少都会一次性读完写到第一个分区文件中，并检查是否大于阈值（这里为6），大于阈值则继续读取下一行并写到下一个文件中，如果小于或等于阈值则接着读取下一行，直至大于阈值（spark读取文件是以行为单位，不会把一行单独拆开放到一个文件中）

### RDD的持久化

我们已经知道job是根据action来划分的，而每调用一次action类型的算子都会创建一个新的 job, 每个 job 总是从它血缘的起始开始计算. 所以, 会发现中间的这些计算过程都会重复的执行.

```scala
object CacheDemo {
    def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
        val sc = new SparkContext(conf)
        val rdd1 = sc.parallelize(Array("ab", "bc"))
        val rdd2 = rdd1.flatMap(x => {
            println("flatMap...")
            x.split("")
        })
        val rdd3: RDD[(String, Int)] = rdd2.map(x => {
            (x, 1)
        })
        rdd3.collect.foreach(println)
        println("-----------")
        rdd3.collect.foreach(println)
    }
```

![image-20201106085400010](C:\Users\yzy\Desktop\日志\image-20201106085400010.png)

会发现在调用第二个collect的时候又把该job的所有流程都计算了一遍，而完全没有这个必要。

所以引入数据持久化，将所需的rdd结果保存起来（可以是内存，硬盘。。。）使得计算效率更快。

可以使用方法persist()或者cache()来持久化一个 RDD. 在第一个 action 会计算这个 RDD, 然后把结果的存储到他的节点的内存中.

```scala
// rdd2.cache() // 等价于 rdd2.persist(StorageLevel.MEMORY_ONLY)
rdd2.persist(StorageLevel.MEMORY_ONLY)
```

![image-20201106085829553](C:\Users\yzy\Desktop\日志\image-20201106085829553.png)

说明:

\1.     第一个 job 会计算 RDD2, 以后的 job 就不用再计算了.

\2.     有一点需要说明的是, 即使我们不手动设置持久化, Spark 也会自动的对一些 shuffle 操作的中间数据做持久化操作(比如: reduceByKey). 这样做的目的是为了当一个节点 shuffle 失败了避免重新计算整个输入. 当时, 在实际使用的时候, 如果想重用数据, 仍然建议调用persist 或 cache\

### 设置检查点

Lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销

为当前 RDD 设置检查点。该函数将会创建一个二进制的文件，并存储到 checkpoint 目录中，该目录是用 SparkContext.setCheckpointDir()设置的。在 checkpoint 的过程中，该RDD 的所有依赖于父 RDD中 的信息将全部被移除。

```scala
package day04

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}


object CheckPointDemo {
    def main(args: Array[String]): Unit = {
        // 要在SparkContext初始化之前设置, 都在无效
        System.setProperty("HADOOP_USER_NAME", "atguigu")
        val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
        val sc = new SparkContext(conf)
        // 设置 checkpoint的目录. 如果spark运行在集群上, 则必须是 hdfs 目录
        sc.setCheckpointDir("hdfs://hadoop201:9000/checkpoint")
        val rdd1 = sc.parallelize(Array("abc"))
        val rdd2: RDD[String] = rdd1.map(_ + " : " + System.currentTimeMillis())

        /*
        标记 RDD2的 checkpoint.
        RDD2会被保存到文件中(文件位于前面设置的目录中), 并且会切断到父RDD的引用, 也就是切断了它向上的血缘关系
        该函数必须在job被执行之前调用.
        强烈建议把这个RDD序列化到内存中, 否则, 把他保存到文件的时候需要重新计算.
         */
        rdd2.checkpoint()
        rdd2.collect().foreach(println)
        rdd2.collect().foreach(println)
        rdd2.collect().foreach(println)
    }
}
```

![image-20201106092312756](C:\Users\yzy\Desktop\日志\image-20201106092312756.png)

![image-20201106092320155](C:\Users\yzy\Desktop\日志\image-20201106092320155.png)

### 持久化和checkpoint的区别

1. 持久化不改变rdd的依赖，而checkpoint改变了rdd的依赖
2. 持久化数据通常保存在内存或硬盘中，而checkpoint数据常保存在hdfs中容错性高
3. 没有持久化，就设置检查点，要将 RDD 的数据写入外部文件系统的话，需要全部重新计算一次，所以建议在设置检查点前先持久化

## RDD编程进阶

spark有三大数据结构：rdd,广播变量，累加器

广播变量：分布式只读共享变量

累加器：分布式只写共享变量

## 集群中运行spark

## standalong模式

首先把在idea中写好的程序通过maven打成jar包，然后到集群环境的master节点下输入命令

```shell
bin/spark-submit 
--class com.hxh.spark.phoneSort 
--master spark://192.168.202.103:7077 
spark01-1.0-SNAPSHOT.jar  phoneFlow out/outputphone
```

class是phoneSort的主类名

master是集群的启动方式local[*]为本地，yarn为yarn启动，standalong模式的话为spark://master节点的ip:7077

然后是jar包的名称，最后两个参数为输入输出路径，在程序中指定

## HDFS

### HDFS客户端操作

#### HDFS客户端环境准备

1．根据自己电脑的操作系统拷贝对应的编译后的hadoop jar包到非中文路径（例如：D:\Develop\hadoop-2.7.2）                  

2．配置HADOOP_HOME环境变量，如图3-5所

图3-5 配置HADOOP_HOME环境变量

\3. 配置Path环境变量，如图3-6所示

图3-6 配置Path环境变量

4．创建一个Maven工程HdfsClientDemo

5．导入相应的依赖坐标+日志添加

```xml
<dependencies>

​    <dependency>

​      <groupId>junit</groupId>

​      <artifactId>junit</artifactId>

​      <version>RELEASE</version>

​    </dependency>

​    <dependency>

​      <groupId>org.apache.logging.log4j</groupId>

​      <artifactId>log4j-core</artifactId>

​      <version>2.8.2</version>

​    </dependency>

​    <dependency>

​      <groupId>org.apache.hadoop</groupId>

​      <artifactId>hadoop-common</artifactId>

​      <version>2.7.2</version>

​    </dependency>

​    <dependency>

​      <groupId>org.apache.hadoop</groupId>

​      <artifactId>hadoop-client</artifactId>

​      <version>2.7.2</version>

​    </dependency>

​    <dependency>

​      <groupId>org.apache.hadoop</groupId>

​      <artifactId>hadoop-hdfs</artifactId>

​      <version>2.7.2</version>

​    </dependency> 

<dependency>

​      <groupId>jdk.tools</groupId>

​      <artifactId>jdk.tools</artifactId>

​      <version>1.8</version>

​      <scope>system</scope>

​      <systemPath>${JAVA_HOME}/lib/tools.jar</systemPath>

  </dependency>

</dependencies>
```



注意：如果Eclipse/Idea打印不出日志，在控制台上只显示

```
1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 

2.log4j:WARN Please initialize the log4j system properly. 

3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入

```properties
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```

6．创建包名：com.atguigu.hdfs

7.创建Java类，在hdfs上创建文件

```java
  //1.获取hdfs客户端对象和配置对象
        Configuration conf = new Configuration();
//        conf.set("fs.defaultFS","hdfs://192.168.202.103:9000");
//        System.setProperty("HADOOP_USER_NAME","hadoop01");
        FileSystem fs = FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");

        //在hdfs上创建路径
        Path path = new Path("/0529/dashen/banzhang");
        fs.mkdirs(path);
        //关闭资源
        fs.close();
        System.out.println("over");
```

#### 上传文件到hdfs

```java
@Test
public void testCopyFromLocalFile() throws URISyntaxException, IOException, InterruptedException {
    //1.获取fs对象和配置
    Configuration conf = new Configuration();
    FileSystem fs =  FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
    //2.上传文件
    fs.copyFromLocalFile(new Path("D:/hello.txt"),new Path("/0529/dashen/banzhang/"));
    fs.close();
    System.out.println("over");

}
```

将hdfs-site.xml拷贝到项目的资源目录下,并配置其副本数

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>dfs.replication</name>
        <value>1</value>
	</property>
</configuration>

```

参数优先级排序：（1）客户端代码中设置的值 >（2）ClassPath下的用户自定义配置文件 >（3）然后是服务器的默认配置

#### 下载文件到本地

```java
 public void testCopyToLocalFile() throws URISyntaxException, IOException, InterruptedException {
        //1.获取fs对象和配置
        Configuration conf = new Configuration();

        FileSystem fs =  FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
        //2.上传文件,此处在我的电脑上必须使用这种形式否则报错
        fs.copyToLocalFile(false,new Path("/0529/dashen/banzhang/hello3.txt"),new Path("D:/hello3.txt"),true);
        fs.close();
        System.out.println("over");
    }
```

关于copyToLocalFile

```java
copyToLocalFile(boolean delSrc, Path src, Path dst, boolean useRawLocalFileSystem)
delSrc : 是否删除src
useRawLocalFileSystem：是否使用RawLocalFileSystem作为本地文件系统。
```

#### 文件的删除

```java
 public void testDelete() throws URISyntaxException, IOException, InterruptedException {
        //1.获取fs对象和配置
        Configuration conf = new Configuration();

        FileSystem fs =  FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
        //2.删除文件,b是指是否递归删除
        fs.delete(new Path("/0529/dashen/banzhang/hello3.txt"),true);
        fs.close();
        System.out.println("over");
    }
```

#### 文件的改名

```java
  @Test
    public void testRename() throws URISyntaxException, IOException, InterruptedException {
        //1.获取fs对象和配置
        Configuration conf = new Configuration();

        FileSystem fs =  FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
        //2.文件改名
        fs.rename(new Path("/0529/dashen/banzhang/hello2.txt"),new Path("/0529/dashen/banzhang/hello4.txt"));
        fs.close();
        System.out.println("over");
    }
```

#### 获取文件信息

```java
@Test
    public void testListFiles() throws IOException, InterruptedException, URISyntaxException{
        // 1获取文件系统
        Configuration configuration = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");

        // 2 获取文件详情
        RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);

        while(listFiles.hasNext()){
            LocatedFileStatus status = listFiles.next();

            // 输出详情
            // 文件名称
            System.out.println(status.getPath().getName());
            // 长度
            System.out.println(status.getLen());
            // 权限
            System.out.println(status.getPermission());
            // 分组
            System.out.println(status.getGroup());

            // 获取存储的块信息
            BlockLocation[] blockLocations = status.getBlockLocations();

            for (BlockLocation blockLocation : blockLocations) {

                // 获取块存储的主机节点
                String[] hosts = blockLocation.getHosts();

                for (String host : hosts) {
                    System.out.println(host);
                }
            }

            System.out.println("-----------班长的分割线----------");
        }
// 3 关闭资源
        fs.close();
```

### HDFS的IO流操作

#### 文件上传

从本地上传文件到hdfs

```java
//1.获取文件系统
        URI uri = new URI("hdfs://192.168.202.103:9000");
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(uri,conf,"hadoop01");

        //2. 创建输入流
        FileInputStream fis = new FileInputStream(new File("d:/hello.txt"));


        //3.创建输出流
        FSDataOutputStream fos = fs.create(new Path("/banzhang1.txt"));
        //4.流对拷
        IOUtils.copyBytes(fis,fos,conf);

        //5.关闭资源
        fis.close();
        fos.close();
        fs.close();
```

#### 文件下载

从hdfs下载文件

```java
 //1.获取文件系统
        URI uri = new URI("hdfs://192.168.202.103:9000");
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(uri,conf,"hadoop01");

        //2. 创建输入流
        FSDataInputStream fis = fs.open(new Path("/banzhang.txt"));

        FileOutputStream fos = new FileOutputStream("d:/banhua.txt");

        IOUtils.copyBytes(fis,fos,conf);
        // 5 关闭资源
        IOUtils.closeStream(fos);
        IOUtils.closeStream(fis);
        fs.close();
```

#### 定位下载大文件

分块读取HDFS上的大文件

```java
@Test
    public void getFirstSeek() throws URISyntaxException, IOException, InterruptedException {
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
        FSDataInputStream fis = fs.open(new Path("/0529/hadoop-2.7.3.tar.gz"));
        FileOutputStream fos = new FileOutputStream("e:/hadoop-2.7.2.tar.gz.part1");
        byte [] bt =new byte[1024];
        for (int i=0;i<1024*128;i++){
            fis.read(bt);
            fos.write(bt);
        }
        IOUtils.closeStream(fis);
        IOUtils.closeStream(fos);
        fs.close();

    }
    @Test
    public void getSecondSeek() throws URISyntaxException, IOException, InterruptedException {
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://192.168.202.103:9000"),conf,"hadoop01");
        FSDataInputStream fis = fs.open(new Path("/0529/hadoop-2.7.3.tar.gz"));
        fis.seek(1024*1204*128);
        FileOutputStream fos = new FileOutputStream("e:/hadoop-2.7.2.tar.gz.part2");
       IOUtils.copyBytes(fis,fos,conf);

        IOUtils.closeStream(fis);
        IOUtils.closeStream(fos);
        fs.close();

    }
```

（3）合并文件

在Window命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并 type hadoop-2.7.2.tar.gz.part2 >> hadoop-2.7.2.tar.gz.part1合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。

停止集群

并将集群中所有的机子

rm -rf data/ logs/

## 序列化

### 序列化概述

**序列化**：就是把内存中的对象，转换成字节序列(或其他数据传输协议)以便于存储(持久化)和网络传输。
**反序列化**：就是将收到字节序列(或其他数据传输协议)或者是硬盘的持久化数据，转换成内存中的对象。

### 自定义bean对象实现序列化接口（Writable）

（1）必须实现Writable接口

（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造

```
   public FlowBean() {    super();  }   
```

（3）重写序列化方法

```java
@Override
public void write(DataOutput out) throws IOException {
	out.writeLong(upFlow);
	out.writeLong(downFlow);
	out.writeLong(sumFlow);
}
```

（4）重写反序列化方法

  ```java
@Override
public void readFields(DataInput in) throws IOException {
	upFlow = in.readLong();
	downFlow = in.readLong();
	sumFlow = in.readLong();
}
  ```

（5）注意反序列化的顺序和序列化的顺序完全一致

（6）要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。

（7）如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。(如下文的按照流量倒叙排序就是实现了WritableComparable接口)

## MapReduce

wordcount一直报错：java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable

### 1.MapReduce的数据输入

#### 1.1 数据切片与MapTask并行度决定机制

1. 一个Job的Map阶段并行度由客户端在提交Job时的切片数决定

2. 每一个Split切片分配-一个MapTask并行实例处理

3. 默认情况下，切片大小=BlockSize（新版本yarn状态下是128m,本地是32m）

4. 切片时不考虑数据集整体，而是逐个针对每一一个 文件单独切片

![image-20201028104807398](C:\Users\yzy\Desktop\日志\image-20201028104807398.png)

![image-20201028104856441](C:\Users\yzy\Desktop\日志\image-20201028104856441.png)

如图所示：这里假设切片大小为128M，则ss.avi会被切成三份（128,128,44）而ss2.avi则会单独被切成一份，而不是和之前的ss第三块组合到一起切片

**数据块：**Block是HDFS物理上把数据分成一块一块。

**数据切片：**数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。（也就是说当切片大小和blocksize不一致的话数据在磁盘上的实际存储还是按照block的大小，只是在切片的时候切成设置的切片大小）

#### 1.2 FileInputFormat切片机制

默认的是TextInputFormat切片机制，除了CombineTextInputFormat切片机制和其不同外其余都基本相同，其具体操作流程的源码解析如下：

**文件切片的源码解析**

```
(1) 程序先找到你数据存储的目录。
(2)开始遍历处理(规划切片)目录下的每一个文件
(3)遍历第一个文件ss.txt 
a)获取文件大小fs sizeOf(ss txt)
b)计算切片大小
computeSliteSize( Math. max( minSize, M ath. min( maxSize, blocksize))=blo cksize= 128 M
 
c)默认情况下,切片大小=blocksize
d)开始切，形成第1个切片: s.tzxt- -0:128M第2个切片stxt- -128:256M第3个切片s.txt- -256M: 300M
(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片)
e)将切片信息写到一个切片规划文件中
f)整个切片的核心过程在getSplit0方法中完成
g) InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。
(4)提交切片规划文件到YARN上，YARN上的MrAppM aster就可以根据切片规划文件计算开启MapTask个数。
```

(1)源码中计算切片大小的公式

```
Math max(minSize, Math min(maxSize, blockSize));
mapreduce.input.fileinputformat.split.minsize=1默认值为1
mapreduce.input.fileiputformatsplit.maxsize= Long.MAXValue默认值Long .MAXValue  
```

因此，默认情况下，切片大小=blocksize
(2) 切片大小设置maxsize(切片最大值) :参数如果调得比blockSize小，则会让切片变   小，而且就等于配置的这个参数的值。
minsize. (切片最小值) :参数调的比blockSize大，则可以让切片变得比blockSize还大。
(3) 获取切片信息API

```
//获取切片的文件名称
string name = inputsplit.getPath() .getName();
//根据文件类型获取切片信息
FileSplit inputSplit = (FileSplit) context . getInputSplit() ;
```

#### 1.3 CombineTextInputFormat切分小文件

对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下

生成切片过程包括：虚拟存储过程和切片过程二部分。

（1）虚拟存储过程：

```
（a）将输入目录下所有文件按照文件名称字典顺序一次读入，记录文件大小，并累加计算所有文件的总长度。

（b）根据是否设置setMaxInputSplitSize值，将每个文件划分成一个一个setMaxInputSplitSize值大小的文件。

（c）注意：当剩余数据大小超过setMaxInputSplitSize值且不大于2倍setMaxInputSplitSize值，此时将文件均分成2个虚拟存储块（防止出现太小切片）。

例如setMaxInputSplitSize值为4M，最后文件剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。
```

（2）切片过程：

```
（a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。

（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。

（c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：
1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）
最终会形成3个切片，大小分别为：
（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M
```

（3）代码实现,在驱动类添加如下代码

```java
  // 如果不设置InputFormat，它默认用的是TextInputFormat.class
        job.setInputFormatClass(CombineTextInputFormat.class);
        //虚拟存储切片最大值设置4m
      CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);
```

#### 1.4FileInputFormat实现类

1. TextInputFormat

   TextInputFormat是默认的InputFormat。每条记录是一行输入。 键是LongWritable类型，存储该行在整
   文件中的起始字节偏移量。值是这行的内容，不包括任何行终止符(换行符和回车符)。

   ![image-20201028114818161](C:\Users\yzy\Desktop\日志\image-20201028114818161.png)

2. KeyValueTextInputFormat

   每一行均为一条记录，被分隔符分割为key,value。可以通过在驱动类中设置conf set(KeyValueLineRecordReader. KEY _VALUE_ SEPERATOR, "t);来设定分隔符。默认分隔符是tab (\t) 。 .![image-20201028115115808](C:\Users\yzy\Desktop\日志\image-20201028115115808.png)

3. NLineInputFormat

   如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按Block块去划分，而是按
   NlineInputFormat指定的行数N来划分。即输入文件的总行数N=切片数，如果不整除，切片数=商+1。

   其（k,v）的取值还是按照默认TextInputFormat的偏移量和值去切分

![image-20201028115247415](C:\Users\yzy\Desktop\日志\image-20201028115247415.png)

#### 1.5自定义InputFormat

自定义InputF orm at步骤如下:
(1)自定义1个类继承FileInputFormat。

(2)改写RecordReader

(3)定义Mapper,Reducer,Driver

**实例**

将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。

1、自定义一个类继承FileInputFormat

```
(1)重写isSplitable()方法， 返回false不可切割
(2)重写createRecordReader()， 创建自定义的RecordReader对	象，并初始化
```

2、改写RecordReader，实现一次读取一个完整文件封装为KV（RecordReader每读取一个文件创建一次，这是因为在本地运行，所以是串行）

```
(1)采用IO流- -次读取一个文件输出到value中， 因为设置了不可切片，最终把所有文件都封装到了value中
(2)获取文件路径信息+名称，并设置key
```

3、设置Driver

```
// (1)设置输入的inputFormat
job.setInputFormatClass(WholeFileInputformat.class);
// (2)设置输出的outputFormatjob.setoutputFormatClass(SequenceFileOutputFormat.class);
```



#### 1.6 总结FileInputFormat的切片和键值机制

FileInputFormat常见的接口实现类包括: TextInputFormat、
KeyValueTextInputFormat、NLineInputFormat、 CombineTextInputFormat和自 定义InputFormat等。

**对于切片机制**：除了CombineTextInputFormat，NLineInputFormat按行切分外其余的都是采用默认的切片机制

**键值对的划分**：除了KeyValueTextInputFormat外，其余都是按偏移量，内容划分

### 2.shuffle机制

是在map之后reduce之前，系统执行排序的过程

在context.writer之后进入collection收集器，收集器通过传入的partion参数确定分区的数量

#### Partition分区

默认分区个数是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。

![image-20201029092604298](C:\Users\yzy\Desktop\日志\image-20201029092604298.png)

key.hashCode() & Integer.MAX_VALUE 是要保证任何map输出的key在numReduceTasks取模后决定的分区为正整数

#### 自定义分区

自定义Partitioner 步骤：

1. 自定义类继承Partitioner，重写getPartition()方法

2. 在Job驱动中，设置自定义Partitioner

   ```
   job.setPatitionerClass(CustomPautitioner.class);
   ```

3. 自定义Pautition后， 要根据自定义Partitioner的逻辑设置相应数量的ReduceTask

   ```
   job. setNumReduce Tasks(5);
   ```

分区总结

1. 当分区数量不为一，ReduceTask为一，则最后所有输出结果都写到一个文件里
2. 当分区数量不为一，ReduceTask不为一，且小于分区数量 则会报错
3. 当分区数量不为一，ReduceTask不为一，且大于分区数量，则会产生空的输出文件

当不报错的情况下，产生的输出文件是由ReduceTask决定的

#### WritableComparable排序

MapTask和ReduceTask均会对数据按照key进行排序。该操作属于
Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。

默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。

**快排的思想**：

1. 在数组中选一个基准数（通常为数组第一个）；

2. 将数组中小于基准数的数据移到基准数左边，大于基准数的移到右边（在左右两边设置两个指针，通过指针移动数据的值，当两个指针位置重合开启下一轮递归）；

3. 对于基准数左、右两边的数组，不断重复以上两个过程，直到每个子集只有一个元素，即为全部有序。

**排序分类**
(1)部分排序
	MapReduce根据输入记录的键对数据集排序。保证输出的每个文	件内部有序。
(2)全排序
	最终输出结果只有一个文件，且文件内部有序。实现方式是只设	置1个ReduceTask。但该方法在 处理大型文件时效率极低，因为	1 台机器处理所有文件，完全丧失了MapReduce所提供的并行架	构。
(3)辅助排序: (Group ingComparator分组)
	Mapreduce框架在记录到达Reducer之前按键对记录排序，但键所对应的值并没有被排序。一般来说 ，大多数MapReduce程序会避免让Reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。
(4)二次排序
	在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。

#### 全排序案例

对总流量进行倒叙排序

(1)FlowBean实现WritableComparable接口重写 CompareTo方法

```java
 @Override
    public int compareTo(flowBean o) {
        int result;

        if(sumFlow>o.getSumFlow()){
            result = -1;
        }else if (sumFlow<o.getSumFlow()){
            result = 1;
        }else {
            result = 0;
        }
        return result;
    }
```

（2）修改Mapper和Reducer的键值

（3）若要实现分区排序则在全排序的基础之上实现一个自定义分区类就行了，继承Partitioner类

```java
public class ProvincePartitioner extends Partitioner<flowBean, Text> {
    @Override
    public int getPartition(flowBean flowBean, Text text, int numPartitions) {
        String preNum = text.toString().substring(0,3);
        int partition = 4;

        if ("136".equals(preNum)){
            partition = 0;
        }else if ("137".equals(preNum)){
            partition = 1;
        }else if("138".equals(preNum)){
            partition = 2;
        }else if ("139".equals(preNum)){
            partition = 3;
        }
        return partition;
    }
}
```

然后在驱动类中加上

```java
job.setPartitionerClass(ProvincePartitioner.class);
job.setNumReduceTasks(5);
```

用于设置分区和reduceTask

#### Combiner合并

Combiner 其实就是对每个MapTask的输出进行局部汇总，减少网络的传输量，然后将其输出的结果输入reduce再进行一次汇总，但使用的前提是不改变最终的输出结果

而Reducer是将所有的结果汇总到一起输出（当reduceTask为1时）

如果combiner的逻辑和Reducer的逻辑是一样的直接在驱动类中加上一行设置即可

```java
// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑
job.setCombinerClass(WordcountReducer.class);
```

如不一样则，单独写一个Combiner写法和Reducer一样

#### GroupingComparator分组（辅助排序）

对Reduce阶段的数据根据某一个或几个字段进行分组。

**实例**

有如下订单数据

| 订单id  | 商品id | 成交金额 |
| ------- | ------ | -------- |
| 0000001 | Pdt_01 | 222.8    |
|         | Pdt_06 | 25.8     |
| 0000002 | Pdt_03 | 522.8    |
|         | Pdt_04 | 122.4    |
|         | Pdt_05 | 722.4    |
| 0000003 | Pdt_07 | 232.8    |
|         | Pdt_02 | 33.8     |

现在需要求出每一个订单中最贵的商品。

如果key值不唯一则需要几个key都相同才行

**分析：**

（1）利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id区分，按照金额排序（倒叙），然后发送到Reduce。

（2）在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是最大值

**步骤**

1. map阶段：自定义一个bean对象，因为需要排序所以继承WritableComparable接口，在其中定义属性，构造器，实现方法（主要是CompareTo方法）
2. reduce阶段：对map阶段传过来的数据进行分组排序，只要id相同就分为一组（认为key相同），让后把这组数据传入reduce

 #### 自定义功能总结：

1. 自定义Bean对象

   当用于传入的数据的key或value的值与默认类型不符时可以自定义对象需要实现Writable接口，如果需要自定义排序需要继承WritableComparable接口此时需要实现compareTo方法

2. 自定义FileInputFormat

   当原有的文件输入格式不能满足需要的时候需要自定义文件的输入格式。如需要一次性读取一整个文件

   (1)自定义1个类继承FileInputFormat。

   (2)定义一个类继承RecordReader，实现其中方法，在nextKeyValue方法中定义所需要的键值和其读入方式

   (3)定义Mapper,Reducer,Driver

3. 自定义分区

   默认的分区是一，如果需要根据自己的需求分区则需要自定义类继承Partitioner，重写getPartition()方法，实现自定义分区。

#### mapTask的工作机制

![image-20201030145443388](C:\Users\yzy\Desktop\日志\image-20201030145443388.png)

1. read阶段

   inputformat 通过RecorderReader去读取一行行的数据，返回相应的k,v数据

2. Map阶段

   接受read阶段传入的数据对其进行逻辑处理，产生新的键值对

3. Collect阶段

   将Map阶段处理好的数据写出数据到outputCollector，在其内部对数据进行分区和排序，并写入环形缓冲区（**第一次排序**）

4. spill溢写阶段

   即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。

   　　**溢写阶段详情：**

      　步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。(是收集阶段的分区排序？)

    　　  步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。**如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作**。

    　　  步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。

5. Combine阶段

   当所有数据处理完之后，maptask对所有临时文件进行一次合并，确保每个Maptask最终只会生成一个文件。（归并排序，将不同溢写文件同一分区的内容写到一起并排序，最终将所有分区的内容写成一个文件，**第二次排序**）

   ​        当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。
   　　在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。
      　让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。

   参考：https://www.cnblogs.com/hyunbar/p/11692651.html

#### Reducetask工作机制

![image-20201030145538339](C:\Users\yzy\Desktop\日志\image-20201030145538339.png)

1. Copy阶段

   **ReduceTask会主动从所有MapTask上远程 拷贝同一分区数据**，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。

2. Merge阶段

   在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多

3. Sort阶段

   在合并的时候对文件进行归并排序，最终形成一个文件，如果设置了分组排序的话，则按设置的分组逻辑将符合逻辑的key看成是相同的key,并将其输入reduce

4. reduce阶段

   对数据进行逻辑处理，通过outputformat将数据输出

### 3. 数据输出OutputFormat

**分类**

1. 文本输出TextOutputFormat
   默认的输出格式是TextOutputFormat,它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toSting方法把它们转换为字符串。

2. SequenceFileOutputFormat
   将SequenceFileOutputForma输出作为后续MapReduce任务的输入，这便是-种好的输出格式，因为它的格式紧凑，很容易被压缩。

3.  自定义OutputFormat

   为了实现控制最终文件的输出路径和输出格式，可以自定义OutputF ormato
   例如:要在一 -个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现。

#### 自定义OutputFormat案例实操

（1）自定义一个OutputFormat类继承FileOutputFormat

​			该类的返回值是RecordWriter对象所以需要自定义一个RecordWriter对象。

（2）编写自定义RecordWriter类，该类继承RecordWriter，用于自定义数据的输出方式

​		首先实现构造函数

```java
public FilterRecorderWriter(TaskAttemptContext job) {

        // 1 获取文件系统
        try {
            fs = FileSystem.get(job.getConfiguration());
            // 2 创建输出文件路径
            Path atguigu = new Path("e:/atguigu.log");
            Path other = new Path("e:/other.log");

            // 3 创建输出流
            agg = fs.create(atguigu);
            others = fs.create(other);
        } catch (IOException e) {
            e.printStackTrace();
        }

    }
```

重写write方法

```java
 @Override
    public void write(Text key, NullWritable value) throws IOException, InterruptedException {
        if (key.toString().contains("atguigu")){
            agg.write(key.toString().getBytes());
        }else {
            others.write(key.toString().getBytes());
        }
    }
```

关闭资源

```java
 @Override
    public void close(TaskAttemptContext context) throws IOException, InterruptedException {
        if (agg!=null){
            agg.close();
        }
        if (others!=null){
            others.close();
        }
    }
```

(3) 重写driver,mapper,reducer

# Scala

## Scala语言的特点

完全面向对象：scala完全面向对象，故scala去掉了java中非面向对象的元素，如static关键字，void类型

 1.static

 scala无static关键字，由object实现类似静态方法的功能（类名.方法名），object关键字和class的关键字定义方式相同，但作用不同。class关键字和java中的class关键字作用相同，用来定义一个类；object的作用是声明一个单例对象，object后的“类名”可以理解为该单例对象的变量名。

 2.void

 对于无返回值的函数，scala定义其返回值类型为Unit类

 */

}

### Scala的反编译

1）object在底层会生成两个类 Hello , Hello$

2）Hello中有个main函数，调用 Hello$ 类的一个静态对象 MODULES$

3）Hello$.MODULE$. 对象时静态的，通过该对象调用Hello$的main函数

4）可以理解我们在main中写的代码在放在Hello$的main，在底层执行Scala编译器做了一个包装

### 变量和数据类型

#### 变量

var | val 变量名 [: 变量类型] = 变量值

1. 在声明/定义一个变量时，可以使用var或者val来修饰，var修饰的变量可改变，val修饰的变量不可改

2. 声明的变量可以不指定具体类型，但是必须要赋值，且类型确定后就不能修改

#### 字符串输出

1）基本语法

（1）字符串，通过+号连接

（2）printf用法：字符串，通过%传值。

（3）字符串，通过$引用

```scala
package com.atguigu.chapter02

object TestCharType {

    def main(args: Array[String]): Unit = {
        var name: String = "jinlian"
        var age: Int = 18

        //（1）字符串，通过+号连接
        println(name + " " + age)

        //（2）printf用法字符串，通过%传值。
        printf("name=%s age=%d\n", name, age)

        //（3）字符串，通过$引用
        println(s"name=$name age=$age")

        println(
            s"""
             name=${name}
             age=${age}
             """
        )
    }
}
```

#### 键盘输入

在编程中，需要接收用户输入的数据，就可以使用键盘输入语句来获取。

1）基本语法

StdIn.readLine()、StdIn.readShort()、StdIn.readDouble()

### 流程控制

#### for循环

```scala
1）基本语法
for(i <- 1 to 3){
  print(i + " ")
}
//i 将会从 1-3 循环，前后闭合

2）基本语法
for(i <- 1 until 3) {
  print(i + " ")
}
println()
（1）这种方式和前面的区别在于i是从1到3-1
（2）即使前闭合后开的范围
```

#### 循环守卫

相当于在循环中加入判断条件，符合条件的才会进入循环

```scala
for(i <- 1 to 3 if i != 2) {

 print(i + " ")

}
println()

```

#### 循环步长

```scala
for (i <- 1 to 10 by 2) {
      println("i=" + i)
    }
```

#### 嵌套循环

```scala
嵌套循环
1）基本语法
for(i <- 1 to 3; j <- 1 to 3) {
    println(" i =" + i + " j = " + j)
}
说明：没有关键字，所以范围后一定要加；来隔断逻辑

2）基本语法
上面的代码等价
for (i <- 1 to 3) {
    for (j <- 1 to 3) {
        println("i =" + i + " j=" + j)
    }
}
```

####  引入变量

```scala
1）基本语法

for(i <- 1 to 3; j = 4 - i) {

  println("i=" + i + " j=" + j)

}

**说明：**

（1）for推导式一行中有多个表达式时，所以要加**；**来隔断逻辑

（2）for推导式有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当包含多个表达式时，一般每行一个表达式，并用花括号代替圆括号，如下

for {
    i <- 1 to 3
	j = 4 - i
} {
  println("i=" + i + " j=" + j)
}
2）案例实操
上面的代码等价于

for (i <- 1 to 3) {
   var j = 4 - i
   println("i=" + i + " j=" + j)
 }
```

#### 循环返回值

```scala
1）基本语法

val res = for(i <- 1 to 10) yield i

println(res)

说明：将遍历过程中处理的结果返回到一个新Vector集合中，使用yield关键字

2）案例实操

​    需求：将原数据中所有值乘以2，并把数据返回到一个新的集合中。
object TestFor {
  def main(args: Array[String]): Unit = {
​    var res = for( i <-1 to 10 ) yield {
​      i * 2
​    }
​    println(res)
  }
}
```

#### 循环中断

（1）break：breakable放在循环外

（2）continue：breakable放在循环内

```scala
import util.control.Breaks._
object TestBreak {
    def main(args: Array[String]): Unit = {
       var n = 0
        while (n < 10) {
            breakable {
                n += 1
                if (n % 2 != 0) {
                    println(n)
                } else {
                    println("continue")
                    break()
                }
            }
        }
    }
}
即，break只能跳出到breakble包裹的代码外的那一层，如果breakble在循环外就代表跳出循环（break），在循环内则为跳出本次循环
```

### 函数

![image-20201103102945355](C:\Users\yzy\Desktop\日志\image-20201103102945355.png)

#### 函数参数

（1）可变参数，在末尾加上*

（2）如果参数列表中存在多个参数，那么可变参数一般放置在最后

（3）参数默认值，如

```scala
 def test3( name : String, age : Int = 30 ): Unit = {
            println(s"$name, $age")
        }
```

（4）带名参数

#### 函数至简原则

函数至简原则：能省则省

1）至简原则细节

​    （1）return可以省略，Scala会使用函数体的最后一行代码作为返回值

（2）返回值类型如果能够推断出来，那么可以省略

（3）如果函数体只有一行代码，可以省略花括号

（4）如果函数无参，则可以省略小括号。若定义函数时省略小括号，则调用该函数时，也需省略小括号；若定时函数时未省略，则调用时，可省可不省。

（5）如果函数明确声明Unit，那么即使函数体中使用return关键字也不起作用

（6）Scala如果想要自动推断无返回值，可以省略等号

（7）如果不关心名称，只关系逻辑处理，那么函数名（def）可以省略

（8）如果函数明确使用return关键字，那么函数返回就不能使用自行推断了，需要声明返回值类型

```scala
object TestFunction {

    def main(args: Array[String]): Unit = {

        // 0）函数标准写法
        def f1( s : String ): String = {
            return s + " jinlian"
        }
        println(f1("Hello"))

        // 至简原则:能省则省

        //（1） return可以省略,scala会使用函数体的最后一行代码作为返回值
        def f2( s : String ): String = {
            s + " jinlian"
        }
        println(f2("Hello"))

        // 如果函数名使用return关键字，那么函数就不能使用自行推断了,需要声明返回值类型
        /*
        def f22(s:String)={
            return "jinlian"
        }
        */
        
        //（2）返回值类型如果能够推断出来，那么可以省略
        def f3( s : String ) = {
            s + " jinlian"
        }

        println(f3("Hello"))

        //（3）如果函数体只有一行代码，可以省略花括号
        //def f4(s:String) = s + " jinlian"
        //def f4(s:String) = "jinlian"
        def f4() = " dalang"

        // 如果函数无参，但是声明参数列表，那么调用时，小括号，可加可不加。
        println(f4())
        println(f4)

        //（4）如果函数没有参数列表，那么小括号可以省略,调用时小括号必须省略
        def f5 = "dalang"
        // val f5 = "dalang"

        println(f5)

        //（5）如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用
        def f6(): Unit = {
            //return "abc"
            "dalang"
        }
        println(f6())

        //（6）scala如果想要自动推断无返回值,可以省略等号
        // 将无返回值的函数称之为过程
        def f7() {
            "dalang"
        }
        println(f7())

        //（7）如果不关心名称，只关系逻辑处理，那么函数名（def）可以省略
        //()->{println("xxxxx")}
        val f = (x:String)=>{"wusong"}

        // 万物皆函数 : 变量也可以是函数
        println(f("ximenqing"))

        //（8）如果函数明确使用return关键字，那么函数返回就不能使用自行推断了,需要声明返回值类型
        def f8() :String = {
            return "ximenqing"
        }
        println(f8())

    }
}
```

#### 匿名函数

```scala
object TestFunction {
    //高阶函数————函数作为参数
    def calculator(a: Int, b: Int, operator: (Int, Int) => Int): Int = {
        operator(a, b)
    }
    //函数————求和
    def plus(x: Int, y: Int): Int = {
        x + y
    }
    def main(args: Array[String]): Unit = {
        //函数作为参数
        println(calculator(2, 3, plus))
        //匿名函数作为参数
        println(calculator(2, 3, (x: Int, y: Int) => x + y))
        //匿名函数简写形式
        println(calculator(2, 3, _ + _))
    }
}
```

#### 函数柯里化

柯里化(Currying)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数为参数的函数。

```scala
首先我们定义一个函数:
def add(x:Int,y:Int)=x+y
现在我们把这个函数变一下形：
def add(x:Int)(y:Int) = x + y
实际上是依次调用两个普通函数（非柯里化函数），第一次调用使用一个参数 x，返回一个函数类型的值，第二次使用参数y调用这个函数类型的值。
实质上最先演变成这样一个方法：
def add(x:Int)=(y:Int)=>x+y

我们可以调用add方法
add(1)(2)返回结果3
也可以分开调用
add(1)
add(2)
个人感觉这结合了函数的闭包思想
```

#### 惰性求值

1）说明

当**函数返回值被声明为****lazy****时**，函数的**执行将被推迟**，直到我们**首次对此取值，该函数才会执行**。这种函数我们称之为惰性函数。

### 类和对象

#### 特质

Scala语言中，采用特质trait（特征）来代替接口的概念，也就是说，多个类具有相同的特征（特征）时，就可以将这个特质（特征）独立出来，采用关键字trait声明。

Scala中的trait中即**可以有抽象属性和方法，也可以有具体的属性和方法**，**一个类可以混入（mixin）多个特质**。

Scala引入trait特征，第一可以替代Java的接口，第二个也是对单继承机制的一种补充。

（1）特质可以同时拥有抽象方法和具体方法

（2）一个类可以混入（mixin）多个特质

（3）所有的Java接口都可以当做Scala特质使用

（4）**动态混入**：可灵活的扩展类的功能

**（4.1**）动态混入：创建对象时混入**trait**，而无需使类混入该**trait**

（4.2）如果混入的trait中有未实现的方法，则需要实现

```scala
trait PersonTrait {
  //（1）特质可以同时拥有抽象方法和具体方法
  // 声明属性
  var name: String = _
  // 抽象属性
  var age: Int
  // 声明方法
  def eat(): Unit = {
    println("eat")
  }
  // 抽象方法
  def say(): Unit
}
trait SexTrait {
  var sex: String
}
//（2）一个类可以实现/继承多个特质
//（3）所有的Java接口都可以当做Scala特质使用
class Teacher extends PersonTrait with java.io.Serializable {
  override def say(): Unit = {
    println("say")
  }
  override var age: Int = _
}
object TestTrait {
  def main(args: Array[String]): Unit = {
    val teacher = new Teacher
    teacher.say()
    teacher.eat()
    //（4）动态混入：可灵活的扩展类的功能
    val t2 = new Teacher with SexTrait {
      override var sex: String = "男"
    }
    //调用混入trait的属性
    println(t2.sex)
  }
}
```

#### 特质叠加

由于一个类可以混入（mixin）多个trait，且trait中可以有具体的属性和方法，若混入的特质中具有相同的方法（方法名，参数列表，返回值均相同），必然会出现继承冲突问题。冲突分为以下两种：

第一种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait之间没有任何关系，解决这类冲突问题，直接在类（Sub）中重写冲突方法。                       

第二种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait继承自相同的trait（TraitC），及所谓的“钻石问题”，解决这类冲突问题，Scala采用了**特质叠加**的策略。

```scala
trait Ball {
  def describe(): String = {
    "ball"
  }
}
trait Color extends Ball {
  override def describe(): String = {
    "blue-" + super.describe()
  }
}
trait Category extends Ball {
  override def describe(): String = {
    "foot-" + super.describe()
  }
}
class MyBall extends Category with Color {
  override def describe(): String = {
    "my ball is a " + super.describe()
  }
}
object TestTrait {
  def main(args: Array[String]): Unit = {
    println(new MyBall().describe())
  }
}
```

![image-20201103123334109](C:\Users\yzy\Desktop\日志\image-20201103123334109.png)

![image-20201103123409578](C:\Users\yzy\Desktop\日志\image-20201103123409578.png)

（1）案例中的super，不是表示其父特质对象，而是表示上述叠加顺序中的下一个特质，即，**MyClass**中的super**指代Color**，Color**中的super**指代Category**，Category**中的super**指代Ball**。Category和Color谁放在后面先叠加谁

（2）如果想要调用某个指定的混入特质中的方法，可以增加约束：super[]，例如super[Category].describe()。

# geospark

## 样例分析

### 空间范围查询

## RangeQuery范围查询

查询PointRDD中一个范围内的点，GeoSpark对索引过的数据与不带索引的数据做到了两种实现。

```scala
// 不带索引查询
val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)
val considerBoundaryIntersection = false // Only return gemeotries fully covered by the window
val usingIndex = false
var queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, considerBoundaryIntersection, usingIndex)
```

对于没有空间索引的点数据，直接基于过滤算子，在RangeFilter类中判断点是否在范围内。

```scala
// 构建空间索引并利用索引查询
val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)
val considerBoundaryIntersection = false // Only return gemeotries fully covered by the window
val buildOnSpatialPartitionedRDD = false // Set to TRUE only if run join query
spatialRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)
val usingIndex = true
var queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, considerBoundaryIntersection, usingIndex)
```

对于有空间索引的点数据（如四叉树索引），首先对每个分区进行索引编制，在mapPartition算子中将所有的点插入到STR-tree或Quad-tree中，结果存入索引indexedRawRDD。以分区为单位多indexedRawRDD做mapPartition操作。

参考文章：https://extendswind.top/posts/technical/geospark_range_query_code_analysis/

## 实现算法

### 查找停留位置

使用给定的时间和距离阈值来查找移动对象已停止或停留的位置

#### 数据格式

1. 点数据
2. 有时间属性
3. 每个轨迹有唯一标识

#### 轨迹的分割方式

时间分割：两个输入之间的时间大于所设置的值则分割轨迹为两个停留

时间界限分割：以定义的时间界限对轨迹进行分割，如分割时间为一天则从每天凌晨十二点都会被分割

距离分割：输入数据大于指定距离则被分割

#### 方法参数

| 参数                                               | 说明                                                         | 数据类型    |
| -------------------------------------------------- | ------------------------------------------------------------ | ----------- |
| input_features                                     | 可从中找到停留的点轨迹。输入必须为启用时间的图层，并具有用于表示时刻的要素。 | Feature Set |
| output_name                                        | 输出要素服务的名称。                                         | String      |
| track_fields[track_fields,...]                     | 将用于标识唯一轨迹的一个或多个字段。                         | Field       |
| distance_method                                    | 指定如何计算停留要素之间的距离。GEODESIC — 如果空间参考可以平移，则轨迹将在适当的时候穿过国际日期变更线。如果空间参考不可平移，则轨迹将被限制在坐标系的范围之内且不可环绕。PLANAR —将使用平面距离。 | String      |
| distance_tolerance                                 | 点之间将视为一个停留位置的最大距离。                         | Linear Unit |
| time_tolerance                                     | 将视为单个停留位置的最短持续时间。在查找停留时，系统会同时考虑时间和距离。**距离容差**参数将指定距离。 | Time Unit   |
| output_type                                        | 用于指定将如何输出停留要素。DWELL_FEATURES — 将返回构成停留的所有输入点要素。DWELL_MEAN_CENTERS — 将返回表示每个停留组的平均中心的点。这是默认设置。DWELL_CONVEX_HULLS — 将返回表示每个停留组的凸包的面。ALL_FEATURES — 将返回所有输入点要素。 | String      |
| summary_statistics[summary_statistics,...]（可选） | 将根据指定字段进行计算的统计数据。COUNT - 非空值的数目。可用于数值字段或字符串。[null, 0, 2]的计数为 2。SUM - 字段内数值的总和。[null, null, 3] 的总和为 3。MEAN - 数值的平均值。[0,2, null] 的平均值为 1。MIN - 数值字段的最小值。[0, 2, null] 的最小值为 0。MAX - 数值字段的最大值。[0, 2, null] 的最大值为 2。STDDEV - 数值字段的标准差。[1]的标准差为 null。[null, 1,1,1] 的标准差为 null。VAR - 轨迹中数值字段内数值的方差。[1] 的方差为 null。[null, 1,1,1] 的方差为 null。RANGE - 数值字段的范围。其计算方法为最大值减去最小值。[0, null, 1] 的范围为 1。[null, 4] 的范围为 0。ANY - 字符串型字段中的示例字符串。FIRST - 轨迹中指定字段的第一个值。适用于 ArcGIS Enterprise10.8.1。LAST - 轨迹中指定字段的最后一个值。适用于 ArcGIS Enterprise10.8.1。 | Value Table |
| data_store（可选）                                 | 指定将用于保存输出的 ArcGIS Data Store。默认为 SPATIOTEMPORAL_DATA_STORE。存储在 SPATIOTEMPORAL_DATA_STORE 中的所有结果都将存储于 WGS84 中。存储在 RELATIONAL_DATA_STORE 中的所有结果都将保持各自的坐标系。SPATIOTEMPORAL_DATA_STORE—输出将存储在时空大数据存储中。这是默认设置。RELATIONAL_DATA_STORE —输出将存储在关系数据存储中。 | String      |
| time_boundary_split（可选）                        | 用于分割输入数据以进行分析的时间跨度。您可通过时间界限分析定义的时间跨度内的值。例如，如果您使用 1 天的时间界限，并将时间界限参考设置为 1980 年 1 月 1 日，则轨迹将在每天开始时被分割。 | Time Unit   |
| time_boundary_reference（可选）                    | 用于分割输入数据以进行分析的参考时间。将为整个数据跨度创建时间界限，且不需要在开始时产生参考时间。如果未使用参考时间，则将使用 1970 年 1 月 1 日。 | Date        |

#### 输出形式

1. 所有要素，对所有点加一个属性，属于停留点的为1，不属于的为0
2. 只保留停留要素，将停留要素保存到一个集合中输出
3. 保留平均中心，保留一个停留中所有点的在时间和空间上的平均中心。生成的的要素有时间类型间隔
4. 输出停留要素的凸包，生成的的要素有时间类型间隔

输出要素要增加的字段：

| 字段名称         | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| count            | 位于停留中的要素数                                           |
| dwellid          | 要素所属的停留的唯一 ID                                      |
| meanx            | 构成停留的 x 坐标的平均值                                    |
| meany            | 构成停留的 y 坐标的平均值                                    |
| meandistance     | 停留位置中连续点之间的平均距离                               |
| instant_datetime | 当输出类型为停留要素、平均中心或所有要素时，创建单个要素的时间 |
| start_datetime   | 输出类型为凸包时创建的开始时间                               |
| end_datetime     | 输出类型为凸包时创建的结束时间                               |

#### 实现思路

1. 用原生的spark自己实现

   先处理数据再对数据使用spacalrdd

   从文件中读取数据为rdd

   自定义一个Javabean封装有用的数据,加一个额外的字段，记录是否为停留点

   对读取的数据进行封装成为相应类型的rdd

   对封装后的数据进行处理

   （1）使用map，先定义一个list用于储存符合条件的记录

   直接使用过滤行不

   或是先计算出相邻两条记录间的距离保存在前一条记录的相应字段中，（还是不好获取下一条记录的值）

   如果距离大于阈值则分开

2. 使用sparksql

3. geosparksql

#### 问题总结

1. 数据点的采集时间过于密集应该咋办？
2. 可通过case语句选择计算停留点的模式（考虑时间，距离，是否对点进行抽稀）

交谈结果：

1. 使用sparksql+geosparksql实现这个功能可以使用join连接查询实现相邻记录的拼接用于计算。最后结果可以是dateset格式的

2. 用mapgis的featurerdd输入输出并实现这个功能

不用sql的话join怎么办

不用join的话用原始的，我之前写的可以用吗，文件分割的话同一个停留点被分割咋办

InputFeatureLayer可以读入csv数据吗

map,zip（可以考虑）,join,....

人为设置一个分界点，临界值不计算，

# 日志

12.8

下一步一定要搞清楚整个大数据项目的流程，各部分的组件的功能

学习要坚持反复，且要做题思考记忆才会更加深刻

